# -*- coding: utf-8 -*-
"""Detecção_de_Fraudes_com_Análise_de_Dados_e_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f0a7BhDA9xtZUfE8YsDqsvc6VxPu7R84

# **Projeto de Parceria** | Detecção de Fraudes Corporativas com Análise de Dados

Tópicos / Índice

# **Tópicos**

<ol type="1">
  <li>Bibliotecase Recursos adicionais;</li>
  <li>Coleta de Dados;</li>
  <li>Tratamento dos Dados;</li>
  <li>Exploração dos Dados;</li>
  <li>Aprendizado de Máquina;</li>
  <li>Avaaliação do Modelo;</li>
  <li>Visualização de Dados;</li>
  <li>Conclusões.</li>
</ol>

# **Contexto / Escopo do projeto**

Escopo do Projeto de Detecção de Fraudes Corporativas com Análise de Dados

**Objetivo:**

Este projeto tem como objetivos:
- Demonstrar os conhecimentos e habilidades desenvolvidos no curso de Análise de Dados da EBAC.
- Desenvolver um modelo de detecção de fraudes corporativas utilizando técnicas de análise de dados e aprendizado de máquina. O modelo será treinado para identificar padrões suspeitos em transações financeiras e classificá-las como fraudulentas ou não fraudulentas.

**Contexto:**

As fraudes corporativas representam um desafio significativo para as empresas, causando perdas financeiras e danos à reputação. A detecção precoce de fraudes é crucial para mitigar esses riscos e proteger os ativos da empresa.

**Metodologia:**

O projeto seguirá as seguintes etapas:

1. **Coleta e Preparação de Dados:** Os dados de transações financeiras serão coletados e preparados para análise. Isso inclui a limpeza, transformação e seleção de recursos relevantes.

2. **Análise Exploratória de Dados:** Os dados serão explorados para identificar padrões, tendências e anomalias que possam indicar fraudes. Técnicas de visualização de dados serão utilizadas para auxiliar na compreensão dos dados.
3. **Desenvolvimento do Modelo:** Um modelo de aprendizado de máquina será treinado com os dados preparados. Diferentes algoritmos serão avaliados para selecionar o modelo com melhor desempenho na detecção de fraudes.
4. **Avaliação do Modelo:** O modelo será avaliado utilizando métricas relevantes, como precisão, revocação e pontuação F1. A capacidade do modelo de generalizar para novos dados será testada.
5. **Implantação e Monitoramento:** O modelo poderá ser implantado para uso em tempo real, monitorando transações e sinalizando atividades suspeitas. O modelo será atualizado periodicamente para garantir sua eficácia.

**Resultados Esperados:**

Espera-se que o projeto resulte em um modelo de detecção de fraudes corporativas preciso e eficaz, capaz de identificar transações fraudulentas em tempo real e auxiliar na prevenção de perdas financeiras.

**Limitações:**

O modelo será desenvolvido com base nos dados disponíveis. A qualidade e a representatividade dos dados podem influenciar o desempenho do modelo. Além disso, o modelo pode não ser capaz de detectar todos os tipos de fraudes, especialmente aquelas que utilizam métodos sofisticados.

# **1.Bibliotecas e Recursos adicionais**
"""

!pip install kaggle -q

import csv

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report

"""# **2.Coleta de dados**"""

# Faz a API no kaggle, importa o arquivo Json e seleciona-o abaixo.
from google.colab import files
files.upload()

!mkdir - p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Verificar Datasets recentes disponíveis:
!kaggle datasets list

def search_kaggle_dataset():
    """Prompts the user for a search term and searches Kaggle for a dataset.

    Prints the results of the search.
    """

    search_term = input("Enter a search term: ")

    from kaggle.api.kaggle_api_extended import KaggleApi
    api = KaggleApi()
    api.authenticate()

    datasets = api.dataset_list(search=search_term)

    if datasets:
        print(f"Datasets found for '{search_term}':")
        for dataset in datasets:
            print(f"- {dataset.ref}: {dataset.title}")
    else:
        print(f"No datasets found for '{search_term}'.")

# Para esse projeto, foi usado o termo "Fraud Detection"
search_kaggle_dataset()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/
#!kaggle datasets download -d <nome-do-usuario>/<nome-do-dataset>
!kaggle datasets download -d ealaxi/paysim1

!unzip paysim1.zip -d dados

pd.read_csv('/content/dados/PS_20174392719_1491204439457_log.csv')

"""## 2.1 Resumo das informações contida nos dados
 O conjunto de dados contém informações sobre transações financeiras. Cada linha representa uma transação individual e cada coluna representa um atributo da transação.
Os atributos incluem:
 - step: Representa uma unidade de tempo no mundo real. Neste caso, 1 passo é 1 hora de tempo. Total de etapas 744 (30 dias de simulação).
 - type: Tipo de transação, como CASH-IN, CASH-OUT, DEBIT, PAYMENT e TRANSFER.
 - amount: O valor da transação.
 - nameOrig: Cliente que iniciou a transação.
 - oldbalanceOrg: Saldo inicial antes da transação.
 - newbalanceOrig: Novo saldo após a transação.
 - nameDest: Cliente que é o destinatário da transação.
 - oldbalanceDest: Saldo inicial do destinatário antes da transação.
 - newbalanceDest: Novo saldo do destinatário após a transação.
 - isFraud: Transações fraudulentas são rotuladas como 1 e transações não fraudulentas são rotuladas como 0.
 - isFlaggedFraud: O modelo de negócios visa controlar transferências massivas de uma conta para outra e sinaliza transferências com mais de 200.000 em uma etapa. As transações fraudulentas sinalizadas são rotuladas como 1 e as transações não sinalizadas como 0.

O conjunto de dados será convertido em um DataFrame que pode ser usado para analisar padrões de transações, identificar transações fraudulentas e desenvolver modelos para detecção de fraudes.

# **3. Tratamento dos Dados**
"""

#Synthetic Financial Datasets For Fraud Detection
dataset_df = pd.read_csv('/content/dados/PS_20174392719_1491204439457_log.csv')

dataset_df.head()

# Renomear colunas usando um dicionário
dataset_df = dataset_df.rename(columns={
    'step': 'periodo_hora',
    'type': 'tipo_transacao',
    'amount': 'valor_transacao',
    'nameOrig': 'cliente_origem',
    'oldbalanceOrg': 'saldo_antigo_origem',
    'newbalanceOrig': 'novo_saldo_origem',
    'nameDest': 'cliente_destino',
    'oldbalanceDest': 'saldo_antigo_destino',
    'newbalanceDest': 'novo_saldo_destino',
    'isFraud': 'e_fraude',
    'isFlaggedFraud': 'e_fraude_sinalizada'
})

# Exibir as primeiras linhas para verificar as alterações
print(dataset_df.head())

dados_df = dataset_df.copy()

dados_df.info()
display(dados_df.isnull().sum())

# Se precisasse limpar os dados nulos:
#dados_df = dados_df.dropna()
#dados_df.isnull().sum()

"""# **4.Exploração dos Dados**

## 4.1 Variáveis Categóricas
"""

dados_df.describe(include='object').T

unique_clients = dados_df['tipo_transacao'].unique()
print(unique_clients)
unique_clients.shape

dados_df['tipo_transacao'] = dados_df['tipo_transacao'].replace({
    'PAYMENT': 'PAGAMENTO',
    'TRANSFER': 'TRANSFERENCIA',
    'CASH_OUT': 'SAQUE',
    'DEBIT': 'DEBITO',
    'CASH_IN': 'DEPOSITO'
})

# Para verificar as alterações
print(dados_df['tipo_transacao'].value_counts())

unique_clients = dados_df['cliente_origem'].unique()
unique_clients.shape

unique_clients = dados_df['cliente_destino'].unique()
unique_clients.shape

"""### 4.1.1 Tipo de Transação"""

dados_df.tipo_transacao.value_counts()

tipo_transacao = dados_df['tipo_transacao'].value_counts()
transaction = tipo_transacao.index
quantity = tipo_transacao.values

colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0']

fig, ax = plt.subplots(figsize=(12, 8))
wedges, texts, autotexts = ax.pie(quantity,
                                  labels=transaction,
                                  autopct='%1.1f%%',
                                  startangle=90,
                                  colors=colors,
                                  textprops={'fontsize': 12},
                                  shadow=True,
                                  explode=(0.05, 0.05, 0.05, 0.05, 0.05)
                                  )

ax.set_title('Distribuição dos Tipos de Transações',
             fontsize=14, fontweight='bold')
ax.legend(wedges, transaction, title="Tipo de Transação",
          loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
plt.show()

cliente_tipo_transacao = dados_df.groupby(['cliente_origem', 'tipo_transacao'])[
    'tipo_transacao'].count().reset_index(name='count')

cliente_tipo_transacao_max = cliente_tipo_transacao.loc[cliente_tipo_transacao.groupby(
    'cliente_origem')['count'].idxmax()]

segmentacao_clientes = cliente_tipo_transacao_max[['cliente_origem', 'tipo_transacao']]
segmentacao_clientes = segmentacao_clientes.rename(
    columns={'tipo_transacao': 'tipo_transacao_mais_frequente'})

print(segmentacao_clientes)

"""Este DataFrame pode ser para análises posteriores, como:
- Calcular estatísticas descritivas para cada segmento
- Visualizar a distribuição dos segmentos
- Construir modelos de machine learning para cada segmento

### 4.1.2 Identificação do Cliente
"""

limite_categorias = 20

frequencia_clientes = dados_df['cliente_origem'].value_counts()

clientes_mais_frequentes = frequencia_clientes.head(limite_categorias)

outros = frequencia_clientes.iloc[limite_categorias:].sum()

clientes_agrupados = pd.concat(
    [clientes_mais_frequentes, pd.Series([outros], index=['Outros'])])

plt.figure(figsize=(12, 6))
plt.bar(clientes_agrupados.sort_values(ascending=False).index,
        clientes_agrupados.sort_values(ascending=False).values)  # Ordenar as barras
plt.xlabel('Clientes')
plt.ylabel('Frequência')
plt.title('Clientes Mais Frequentes (com Limite de Categorias)')
plt.xticks(rotation=45, ha='right')
plt.yscale('log')
plt.tight_layout()

for i, barra in enumerate(plt.gca().patches):
    if i < 16:
        barra.set_color('red')

print(clientes_agrupados)
print(f"Total de clientes: {frequencia_clientes.sum()}")
print(f"Total de clientes agrupados: {clientes_agrupados.sum()}")
print(
    f"Total de clientes não agrupados: {frequencia_clientes.sum() - clientes_agrupados.sum()}")
print(f"Cliente com maior frequência: {frequencia_clientes.idxmax()}")
print(f"Cliente com menor frequência: {frequencia_clientes.idxmin()}")

"""## 4.2 Variáveis Numéricas"""

dados_df.describe().T

for col in dados_df.select_dtypes(include=['object']).columns:
    print(f'Valores únicos da coluna {col}: {dados_df[col].unique()[:20]}')
    print('----')
    print(
        f'Quantidade de valores únicos da coluna {col}: {len(dados_df[col].unique())}')
    print('----')
    print(
        f'Quantidade de valores nulos na coluna {col}: {dados_df[col].isnull().sum()}')
    print('----')
    print(
        f'Quantidade de valores duplicados na coluna {col}: {dados_df[col].duplicated().sum()}')
    print('----')

"""### 4.2.1 Step (período de 1 hora)"""

plt.figure(figsize=(10, 6))
plt.hist(dados_df['periodo_hora'], bins=24)
plt.xlabel('Período de 1 hora (step)')
plt.ylabel('Número de Transações')
plt.title('Distribuição de Transações ao Longo do Tempo')
plt.grid(True)
plt.show()

dados_df['datetime'] = pd.to_datetime(dados_df['periodo_hora'], unit='h')
dados_agrupados = dados_df.groupby(dados_df['datetime'].dt.date)[
    'valor_transacao'].mean().reset_index()
dados_agrupados.columns = ['Dia', 'Valor Médio da Transação']

plt.figure(figsize=(15, 6))
sns.barplot(x='Dia', y='Valor Médio da Transação', data=dados_agrupados)
plt.title('Valor Médio da Transação por Dia')
plt.xticks(rotation=45, ha='right')

ax = plt.gca()
ax.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m'))

plt.grid(True)
plt.tight_layout()
plt.show()

"""### 4.2.2 Amount (O valor da transação)"""

plt.figure(figsize=(10, 6))
plt.hist(dados_df['valor_transacao'], bins='auto', color='skyblue', edgecolor='black')
plt.xlabel('Valor da Transação (valor_transacao)')
plt.ylabel('Frequência (escala logarítmica)')
plt.title('Distribuição dos Valores das Transações (Escala Logarítmica)')
plt.grid(True)
plt.yscale('log')
plt.xlim(0, 1000000)
plt.show()

plt.figure(figsize=(12, 8))
sns.boxplot(x=dados_df['valor_transacao'], color='lightblue', showmeans=True,
            meanprops={"marker": "o",
                       "markerfacecolor": "white",
                       "markeredgecolor": "black",
                       "markersize": "10"})  # Removed showfliers=False to show outliers
plt.xlabel('Valor da Transação (valor_transacao) em escala logarítmica', fontsize=12)
plt.title('Distribuição dos Valores das Transações (Boxplot, Escala Logarítmica) com Outliers', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6)
plt.xscale('log') # Apply log scale to the x-axis for better visualization of skewed data

plt.show()

plt.figure(figsize=(12, 8))
coluna_x = 'novo_saldo_origem'
coluna_y = 'valor_transacao'

# Adicionando a coluna 'e_fraude' para colorir os pontos
sns.scatterplot(x=coluna_x, y=coluna_y, data=dados_df,
                hue='e_fraude', alpha=0.5)

plt.xlabel('Novo Saldo na Conta de Origem (Escala Logarítmica)', fontsize=12)
plt.ylabel('Valor da Transação (Escala Logarítmica)', fontsize=12)
plt.title('Gráfico de Dispersão: Novo Saldo vs. Valor da Transação', fontsize=14)

# Adicionando uma legenda para identificar as classes
plt.legend(title='Fraude')

# Ajustando os limites dos eixos para uma melhor visualização (opcional, ajuste conforme necessário)
# plt.xlim(0, 1000000)
# plt.ylim(0, 1000000)

plt.xscale('log')
plt.yscale('log')

plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

coluna_x = 'novo_saldo_origem'
coluna_y = 'valor_transacao'

plt.figure(figsize=(14, 9)) # Aumenta o tamanho da figura
plt.xscale('log')
plt.yscale('log')

# Usando uma paleta de cores diferente e aumentando o tamanho dos pontos (opcional)
sns.scatterplot(x=coluna_x, y=coluna_y, data=dados_df,
                hue='e_fraude', alpha=0.6, s=20, palette='viridis') # s=tamanho do ponto, palette=paleta de cores

plt.xlabel('Novo Saldo na Conta de Origem (Escala Logarítmica)', fontsize=14) # Aumenta o tamanho da fonte do rótulo do eixo x
plt.ylabel('Valor da Transação (Escala Logarítmica)', fontsize=14) # Aumenta o tamanho da fonte do rótulo do eixo y
plt.title('Relação entre Novo Saldo de Origem e Valor da Transação, com Destaque para Fraudes', fontsize=16) # Título mais descritivo e maior

plt.legend(title='É Fraude', fontsize=12, title_fontsize=14) # Melhorando a legenda

# Remova os limites fixos para que o gráfico se ajuste automaticamente aos dados
# plt.xlim(0, 1000000)
# plt.ylim(0, 1000000)

plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

"""### 4.2.3 Saldos antes e depois da transação"""

colunas_saldo = ['saldo_antigo_origem', 'novo_saldo_origem',
                 'saldo_antigo_destino', 'novo_saldo_destino']

for coluna in colunas_saldo:
    # Histograma com escala logarítmica
    plt.figure(figsize=(12, 7)) # Aumenta o tamanho da figura
    sns.histplot(dados_df[coluna], bins=100, kde=True, color='skyblue') # Mais bins e cor diferente
    plt.xlabel(f'{coluna} (Escala Logarítmica)', fontsize=12) # Rótulo com indicação de escala logarítmica
    plt.ylabel('Frequência', fontsize=12)
    plt.title(f'Distribuição de {coluna}', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.xscale('log') # Aplica a escala logarítmica ao eixo x
    plt.show()

    # Boxplot
    plt.figure(figsize=(12, 4)) # Ajusta o tamanho da figura para boxplot
    sns.boxplot(x=dados_df[coluna], showfliers=False, color='lightgreen', # Cor diferente e showfliers=False mantido
                meanprops={"marker": "o", "markerfacecolor": "white", "markeredgecolor": "black"}) # Adiciona marcador para a média
    plt.xlabel(f'{coluna} (Sem Outliers)', fontsize=12) # Rótulo com indicação de sem outliers
    plt.title(f'Distribuição de {coluna} (Boxplot, Sem Outliers)', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    # Opcional: Aplicar escala logarítmica no boxplot se a distribuição for muito assimétrica
    # plt.xscale('log')

    plt.show()

"""## 4.3 Variáveis Binárias"""

fraude_counts = dados_df['e_fraude'].value_counts()
plt.figure(figsize=(8, 8)) # Aumenta o tamanho da figura para um círculo maior
plt.pie(fraude_counts.values,
        labels=['Não Fraudulenta', 'Fraudulenta'],
        autopct='%1.1f%%',
        startangle=140, # Ajusta o ângulo inicial para melhor visualização da menor fatia
        colors=['#66c2a5', '#fc8d62'], # Usa uma paleta de cores mais distinta
        wedgeprops={'edgecolor': 'black'}) # Adiciona uma borda preta às fatias para melhor definição

plt.title('Proporção de Transações Fraudulentas e Não Fraudulentas', fontsize=16) # Título mais descritivo e maior
plt.axis('equal')  # Garante que o gráfico seja um círculo perfeito

plt.show()

"""## 4.4 Combinação de Variáveis

### 4.4.1 Tipo de Transação vs. Valor da Transação
"""

plt.figure(figsize=(12, 7))
sns.set_style("whitegrid")

# Ordenar os dados pelo valor médio da transação
dados_ordenados = dados_df.groupby('tipo_transacao')['valor_transacao'].mean().sort_values(ascending=False).reset_index()

# Criar o gráfico de barras com dados ordenados e paleta de cores
ax = sns.barplot(x='tipo_transacao', y='valor_transacao', data=dados_ordenados, palette='viridis') # Use uma paleta de cores vibrante

plt.xlabel('Tipo de Transação', fontsize=12)
plt.ylabel('Valor Médio da Transação', fontsize=12)
plt.title('Distribuição do Valor Médio das Transações por Tipo', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(fontsize=10)

# Adicionar rótulos de dados (valores médios) acima de cada barra
for p in ax.patches:
    ax.annotate(f'{p.get_height():,.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=9)


sns.despine() # Remove as bordas superiores e direitas (opcional)

plt.tight_layout() # Ajusta o layout para evitar sobreposição
plt.show()

"""### 4.4.2 Tipo de Transação vs. Proporção de Transações Fraudulentas"""

plt.figure(figsize=(12, 7))
sns.set_style("whitegrid")

dados_ordenados_fraude = dados_df.groupby('tipo_transacao')['e_fraude'].mean().sort_values(ascending=False).reset_index()

ax = sns.barplot(x='tipo_transacao', y='e_fraude', data=dados_ordenados_fraude, palette='viridis') # Use uma paleta de cores vibrante

plt.xlabel('Tipo de Transação', fontsize=12)
plt.ylabel('Proporção de Transações Fraudulentas', fontsize=12)
plt.title('Proporção de Transações Fraudulentas por Tipo de Transação', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(fontsize=10)

for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=9)


sns.despine()

plt.tight_layout()
plt.show()

"""### 4.4.3 Período vs. Valor da Transação"""

plt.figure(figsize=(14, 7))
sns.scatterplot(x='periodo_hora', y='valor_transacao', hue='tipo_transacao', data=dados_df, alpha=0.6)
plt.xlabel('Etapa de Tempo (periodo_hora)', fontsize=12)
plt.ylabel('Valor da Transação (valor_transacao) em Escala Logarítmica', fontsize=12)
plt.title('Evolução do Valor das Transações por Tipo ao Longo do Tempo', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6)
plt.yscale('log')

plt.legend(title='Tipo de Transação', loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()
plt.show()

"""### 4.4.4 Período vs. Valor Médio da Transação"""

plt.figure(figsize=(15, 7))
sns.set_style("whitegrid")

# Para este tipo de gráfico, pode ser mais interessante ver a média ou a soma por período de tempo agrupado,
# em vez de plotar cada ponto. Vou calcular a média do valor da transação por 'periodo_hora'.
# Se quiser ver a soma, mude .mean() para .sum()
dados_agrupados_tempo = dados_df.groupby('periodo_hora')['valor_transacao'].mean().reset_index()

sns.lineplot(x='periodo_hora', y='valor_transacao', data=dados_agrupados_tempo, marker='o', color='#4daf4a') # Define uma cor específica para a linha

plt.xlabel('Etapa de Tempo (periodo_hora)', fontsize=12)
plt.ylabel('Valor Médio da Transação', fontsize=12)
plt.title('Valor Médio das Transações ao Longo do Tempo', fontsize=16, fontweight='bold')

# Se houver muitos 'periodo_hora', ajuste o intervalo
max_periodo = dados_agrupados_tempo['periodo_hora'].max()
plt.xticks(np.arange(0, max_periodo + 1, 50))

plt.grid(True, linestyle='--', alpha=0.7)

sns.despine()

plt.tight_layout()

plt.show()

"""# **5.Aprendizado de Máquina**

**Possíveis Modelos**
- **Arvores de Decisões**
1. **Random Forest** – Um modelo de ensemble baseado em árvores de decisão, robusto contra overfitting e que lida bem com desequilíbrios usando pesos ou amostragem balanceada.

2. **LightGBM** – Semelhante ao XGBoost, mas geralmente mais rápido e eficiente em grandes volumes de dados, podendo lidar bem com desbalanceamento.

4. **Isolation Forest** – Um modelo específico para detecção de anomalias que constrói árvores de decisão para isolar observações raras (muito usado para fraudes).

- **Redes Neurais**

3. **Autoencoders** – Um tipo de rede neural não supervisionada que pode aprender a reconstruir dados normais e detectar anomalias (como fraudes) a partir de desvios.
"""

features = ['periodo_hora', 'tipo_transacao', 'valor_transacao', 'saldo_antigo_origem',
            'novo_saldo_origem', 'saldo_antigo_destino', 'novo_saldo_destino']
target = 'e_fraude'

dados_df_encoded = pd.get_dummies(dados_df, columns=['tipo_transacao'], prefix=['tipo_transacao'])
features_encoded = features.copy()
features_encoded.remove('tipo_transacao')
features_encoded.extend(['tipo_transacao_DEPOSITO', 'tipo_transacao_SAQUE', 'tipo_transacao_TRANSFERENCIA',
                         'tipo_transacao_PAGAMENTO', 'tipo_transacao_DEBITO'])

X = dados_df_encoded[features_encoded]
y = dados_df_encoded[target]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

"""# **6. Avaliação do Modelo**

- **Métricas de Avaliação:** Ao avaliar modelos de detecção de fraudes, é importante considerar métricas que levem em conta o desbalanceamento de classes, como precisão, revocação, pontuação F1 e AUC (Area Under the Curve), em vez de depender apenas da acurácia.
"""

y_pred_test = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)
auc = roc_auc_score(y_test, y_pred_test)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"AUC: {auc}")
print(classification_report(y_test, y_pred_test))

cm = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix:")
print(cm)

"""# **7.Visualização dos Dados**"""

plt.figure(figsize=(48, 32))
plot_tree(model.estimators_[0],
          feature_names=X.columns,
          filled=True,
          rounded=True,
          class_names=['Não Fraudulenta', 'Fraudulenta'])
plt.title('Árvore de Decisão do Random Forest (Primeira Árvore)')
plt.show()

num_trees = model.n_estimators
print(f"O modelo Random Forest tem {num_trees} árvores de decisão.")

num_leaves = model.estimators_[0].get_n_leaves()
print(f"A árvore treinada tem {num_leaves} folhas.")

# Escolha uma árvore do RandomForest para visualizar
# Vamos visualizar a primeira árvore (índice 0)
chosen_tree = model.estimators_[0]

# Defina os nomes das classes com base nos valores possíveis de 'e_fraude'
# Se 'e_fraude' é 0 ou 1, os nomes das classes seriam:
class_names = ['Não Fraude', 'Fraude'] # Ou ajuste se tiver outros valores

# Obtenha os nomes das features diretamente do X_train
feature_names = X_train.columns.tolist()

# Crie a figura para a visualização da árvore
plt.figure(figsize=(20,10)) # Ajuste o tamanho conforme necessário para visibilidade

# Gere a visualização da árvore
plot_tree(chosen_tree,
          feature_names=feature_names,
          class_names=class_names,
          filled=True,        # Pinta os nós para indicar a classe majoritária
          rounded=True,       # Arredonda as caixas dos nós
          fontsize=8)         # Ajuste o tamanho da fonte para caber na imagem

plt.title("Visualização de uma Árvore de Decisão do RandomForest (Primeira Árvore)")
plt.show()

"""# **8.Conclusões**

1. **Alta Acurácia, mas com ressalvas:** O modelo apresenta uma acurácia muito alta (99,95%), o que pode parecer excelente à primeira vista. No entanto, em conjuntos de dados desbalanceados, como este (com muito mais transações não fraudulentas do que fraudulentas), a acurácia pode ser enganosa. É importante analisar outras métricas para uma avaliação mais completa.

2. **Boa Precisão e Recall:** O modelo demonstra boa precisão (84,06%), indicando que a maioria das transações classificadas como fraudulentas são realmente fraudulentas. Além disso, o recall (86,93%) também é alto, o que significa que o modelo consegue identificar a maioria das transações fraudulentas reais.

3. **F1-Score Equilibrado:** O F1-Score (85,47%) é uma média harmônica entre precisão e recall, e um valor alto indica um bom equilíbrio entre as duas métricas. Isso reforça a ideia de que o modelo tem um bom desempenho geral na detecção de fraudes.

4. **ROC AUC satisfatória:** A área sob a curva ROC (0,9346) é um indicador da capacidade do modelo de distinguir entre transações fraudulentas e não fraudulentas. Um valor próximo de 1 indica um excelente desempenho, e o valor obtido (0,9346) sugere que o modelo tem uma boa capacidade de discriminação.

5. **Matriz de Confusão Revela Detalhes:** A matriz de confusão fornece uma visão mais detalhada do desempenho do modelo:
- Verdadeiros Negativos (TN): 6370081 - A grande maioria das transações não fraudulentas foi classificada corretamente.
- Falsos Positivos (FP): 1383 - Um número relativamente pequeno de transações não fraudulentas foi classificado incorretamente como fraudulentas.
- Falsos Negativos (FN): 1098 - Um número pequeno de transações fraudulentas foi classificado incorretamente como não fraudulentas.
- Verdadeiros Positivos (TP): 7295 - A maioria das transações fraudulentas foi classificada corretamente.

6. **Foco na Redução de Falsos Negativos:** Apesar do bom desempenho geral, é importante focar na redução dos falsos negativos, pois eles representam as transações fraudulentas que o modelo não conseguiu identificar. Isso pode ser feito ajustando o modelo ou utilizando técnicas de balanceamento de dados.

**Em resumo, o modelo Random Forest apresenta um bom desempenho na detecção de fraudes, com alta precisão, recall e F1-Score. No entanto, é fundamental continuar aprimorando o modelo para minimizar os falsos negativos e garantir a detecção mais completa possível das transações fraudulentas.**
"""